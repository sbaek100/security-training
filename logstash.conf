# 데이터 입력부 - 여러 소스에서 로그를 받습니다
input {
  # Filebeat에서 오는 로그
  beats {
    port => 5044
  }

  # TCP로 직접 들어오는 로그
  tcp {
    port => 50000
    codec => json_lines
  }

  # UDP로 들어오는 시스템 로그
  udp {
    port => 514
    codec => json
  }
}

# 데이터 가공부 - 받은 로그를 분석하고 정리합니다
filter {
  # 시간 정보 파싱
  if [fields][log_type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:timestamp} %{IPORHOST:host} %{DATA:program}(?:\[%{POSINT:pid}\])?: %{GREEDYDATA:message_body}" }
    }

    date {
      match => [ "timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }
  }

  # 웹 로그 파싱
  if [fields][log_type] == "apache" {
    grok {
      match => { "message" => "%{COMMONAPACHELOG}" }
    }
  }
}

# 데이터 출력부 - 처리된 로그를 Elasticsearch에 저장합니다
output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    user => "logstash_internal"
    password => "changeme123"

    # 로그 타입별로 다른 인덱스에 저장
    index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
  }

  # 터미널에도 출력 (디버깅용)
  stdout {
    codec => rubydebug
  }
}
